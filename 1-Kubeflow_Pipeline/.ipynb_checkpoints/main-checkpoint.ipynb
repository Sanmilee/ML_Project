{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3243a454",
   "metadata": {},
   "source": [
    "#### import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee203429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kfp.v2.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "import matplotlib.pyplot as plt\n",
    "from kfp.v2 import compiler, dsl\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform, aiplatform_v1\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    Metrics,\n",
    "    Model,\n",
    "    Output,\n",
    "    OutputPath,\n",
    "    component,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fa558",
   "metadata": {},
   "source": [
    "#### load config file\n",
    "- set global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124f4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "REGION = config['utils']['region']\n",
    "PROJECT_ID = config['utils']['project_id']\n",
    "BUCKET_NAME = config['utils']['bucket_name']\n",
    "SERVICE_ACCOUNT = config['utils']['service_account']\n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "PIPELINE_ROOT = config['utils']['pipeline_root']\n",
    "DATASET = config['utils']['dataset']\n",
    "CONTAINER_IMAGE = config['utils']['container_image']   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9218c97",
   "metadata": {},
   "source": [
    "#### get_dataframe component\n",
    "- load data from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8d5dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset, Output)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def load_data(\n",
    "    bq_table: str, \n",
    "    output_data_path: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Pull conversion data from bigquery and dump to csv in gcs\"\"\"\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    bqclient = bigquery.Client(project=project_number)\n",
    "    table = bigquery.TableReference.from_string(bq_table)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    dataframe = rows.to_dataframe(create_bqstorage_client=True)\n",
    "    dataframe = dataframe.sample(frac=1, random_state=2)\n",
    "    dataframe.to_csv(output_data_path.path + \".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae52803",
   "metadata": {},
   "source": [
    "#### preprocessing component\n",
    "- preprocess dataset for ML operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6e88dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset, Input, Output)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def preprocessing(\n",
    "    dataset: Input[Dataset], \n",
    "    train_dataset: Output[Dataset],\n",
    "    test_dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Preprocess data\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(dataset.path + \".csv\")\n",
    "    df = df.dropna()\n",
    "    \n",
    "    train_data, test_data = train_test_split(df)\n",
    "    \n",
    "    train_data.to_csv(train_dataset.path + \".csv\", index=False)\n",
    "    test_data.to_csv(test_dataset.path + \".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb36860",
   "metadata": {},
   "source": [
    "#### decision_tree_train component\n",
    "- train model using decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cef2bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing decision_tree_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile decision_tree_train.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset, Input, Model, Output)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def decision_tree_train(\n",
    "    train_dataset: Input[Dataset], \n",
    "    model: Output[Model]\n",
    "):\n",
    "    \"\"\"Train decision_tree model on data, dump model\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.tree import DecisionTreeClassifier    \n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    \n",
    "    df = pd.read_csv(train_dataset.path + \".csv\")\n",
    "    X_train = df.iloc[:, :-1]\n",
    "    y_train = df.iloc[:, -1]\n",
    "    \n",
    "    # Preprocess features (categorical & numerical)\n",
    "    # Num features\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    # Cat features\n",
    "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    \n",
    "    # Set target features to preprocess\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Apply predefined transformations  \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "                    ('num', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    \n",
    "    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    \n",
    "    \n",
    "    decision_tree_model = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n",
    "    decision_tree_model.fit(X_train, y_train) \n",
    "    \n",
    "    \n",
    "    model.metadata[\"framework\"] = \"DT\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(decision_tree_model, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3762fa4",
   "metadata": {},
   "source": [
    "#### random_forest_model component\n",
    "- train model using random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66299cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing random_forest_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile random_forest_train.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Dataset, Input, Model, Output)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def random_forest_train(\n",
    "    train_dataset: Input[Dataset], \n",
    "    model: Output[Model]\n",
    "):\n",
    "    \"\"\"Train random_forest model on data, dump model\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestClassifier    \n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    \n",
    "    df = pd.read_csv(train_dataset.path + \".csv\")\n",
    "    X_train = df.iloc[:, :-1]\n",
    "    y_train = df.iloc[:, -1]\n",
    "    \n",
    "    # Preprocess features (categorical & numerical)\n",
    "    # Num features\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    # Cat features\n",
    "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    \n",
    "    # Set target features to preprocess\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Apply predefined transformations  \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "                    ('num', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    \n",
    "    \n",
    "    random_forest_model = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n",
    "    random_forest_model.fit(X_train, y_train) \n",
    "    \n",
    "    \n",
    "    model.metadata[\"framework\"] = \"RF\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(random_forest_model, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1612f57",
   "metadata": {},
   "source": [
    "#### evaluate_model component\n",
    "- get both models' evaluation\n",
    "- determine the best in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a38fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluate_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate_model.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.v2.dsl import (Dataset, Input, Metrics, Model, Output)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def evaluate_model(\n",
    "    test_dataset: Input[Dataset], \n",
    "    dt_model: Input[Model],\n",
    "    rf_model: Input[Model],\n",
    "    metrics: Output[Metrics]\n",
    ") -> NamedTuple(\"output\", [(\"optimal_model\", str)]):\n",
    "    \n",
    "    \"\"\"Evaluate model on test data\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    \n",
    "    def best_model(rf_acc_value, dt_acc_value):\n",
    "        \"\"\"get best model base off accuracy\"\"\"\n",
    "        opt_model = \"random_forest\"\n",
    "        if dt_acc_value > rf_acc_value:\n",
    "            opt_model = \"decision_tree\"\n",
    "        return opt_model\n",
    "  \n",
    "\n",
    "    df = pd.read_csv(test_dataset.path + \".csv\")\n",
    "    X_test = df.iloc[:, :-1]\n",
    "    y_test = df.iloc[:, -1]\n",
    "    \n",
    "     \n",
    "    # decision tree model evaluation\n",
    "    decision_tree_train = DecisionTreeClassifier()\n",
    "    file_name_1 = dt_model.path + \".pkl\"\n",
    "    with open(file_name_1, 'rb') as file:  \n",
    "        decision_tree_train = pickle.load(file)\n",
    "    y_pred = decision_tree_train.predict(X_test)\n",
    "    dt_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    metrics.log_metric(\"dt_accuracy\", round(dt_accuracy, 2))\n",
    "    \n",
    "    \n",
    "    # random forest model evaluation\n",
    "    random_forest_model = RandomForestClassifier()\n",
    "    file_name_2 = rf_model.path + \".pkl\"\n",
    "    with open(file_name_2, 'rb') as file:  \n",
    "        random_forest_model = pickle.load(file)\n",
    "    y_pred = random_forest_model.predict(X_test)\n",
    "    rf_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    metrics.log_metric(\"rf_accuracy\", round(rf_accuracy, 2))\n",
    "    \n",
    "    \n",
    "    optimal_model = best_model(rf_accuracy, dt_accuracy)\n",
    "    return (optimal_model,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bff8c2",
   "metadata": {},
   "source": [
    "#### deploy_model component\n",
    "- deploy the model wih the best evaluation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea6edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_model.py\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from typing import NamedTuple\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "BASE_IMAGE = config['utils']['base_image']\n",
    "\n",
    "@dsl.component(base_image=BASE_IMAGE, install_kfp_package=False,)\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    container_image : str, \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model] \n",
    "):\n",
    "    \"\"\"Train sklearn model on bean data csv, dump model\"\"\"\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    \n",
    "    DISPLAY_NAME  = \"conversion_model\"\n",
    "    MODEL_NAME = \"conversion_model_v1\"\n",
    "    ENDPOINT_NAME = \"conversion_endpoint\"\n",
    "    \n",
    "    \n",
    "    # Create vertex endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_NAME, \n",
    "        project=project, \n",
    "        location=region\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Upload model to vertex model registry\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name = DISPLAY_NAME, \n",
    "        artifact_uri = model.uri.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri =  container_image,\n",
    "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "        serving_container_environment_variables={\"MODEL_NAME\": MODEL_NAME},       \n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=\"n1-standard-4\", \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd99b5",
   "metadata": {},
   "source": [
    "#### modularize components into pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d614a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"ml-pipeline\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_table: str = DATASET,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    container_image: str = CONTAINER_IMAGE\n",
    "):\n",
    "    \n",
    "    from load_data import load_data\n",
    "    from preprocessing import preprocessing\n",
    "    from decision_tree_train import decision_tree_train\n",
    "    from random_forest_train import random_forest_train\n",
    "    from evaluate_model import evaluate_model\n",
    "    from deploy_model import deploy_model\n",
    "     \n",
    "    # execute load_data component\n",
    "    load_data_op = load_data(bq_table)\n",
    "    \n",
    "    # execute preprocessing component\n",
    "    preprocessing_op = preprocessing(load_data_op.output)\n",
    "    \n",
    "    # execute decision_tree_train & random_forest_train components\n",
    "    train_decision_tree_op = decision_tree_train(preprocessing_op.outputs[\"train_dataset\"])\n",
    "    train_random_forest_op = random_forest_train(preprocessing_op.outputs[\"train_dataset\"])\n",
    "    \n",
    "    # execute evaluate_model component\n",
    "    evaluate_model_op = evaluate_model(\n",
    "        preprocessing_op.outputs[\"test_dataset\"],\n",
    "        train_decision_tree_op.output,\n",
    "        train_random_forest_op.output\n",
    "    )\n",
    "    \n",
    "    # set conditional to get best model\n",
    "    # run if random_forest is the best model\n",
    "    with dsl.Condition(\n",
    "        evaluate_model_op.outputs[\"optimal_model\"] == \"random_forest\",\n",
    "        name=\"deploy-model\",\n",
    "    ) as condition:\n",
    "        condition.display_name = \"deploy_random_forest\"\n",
    "        deploy_model_op = deploy_model(\n",
    "            model=train_random_forest_op.outputs['model'],\n",
    "            project=project,\n",
    "            region=region, \n",
    "            serving_container_image_uri = container_image)\n",
    "        \n",
    "    # run if decision_tree is the best model    \n",
    "    with dsl.Condition(\n",
    "        evaluate_model_op.outputs[\"optimal_model\"] == \"decision_tree\",\n",
    "        name=\"deploy-model\",\n",
    "    ) as condition:\n",
    "        condition.display_name = \"deploy_decision_tree\"\n",
    "        deploy_model_op = deploy_model(\n",
    "            model=train_decision_tree_op.outputs['model'],\n",
    "            project=project,\n",
    "            region=region, \n",
    "            serving_container_image_uri = container_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c34aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a JSON file that you'll use to run the pipeline:\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9823c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c14660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vertex pipeline job\n",
    "\n",
    "api_client = aiplatform.PipelineJob(\n",
    "    display_name=\"conversion-model-pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    job_id=f\"conversion-model-pipeline-{TIMESTAMP}\",\n",
    "    enable_caching=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df394936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run vertex pipeline job\n",
    "\n",
    "api_client.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487f079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23468d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
